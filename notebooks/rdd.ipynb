{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark - Работа с RDD (Resilient Distributed Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в RDD\n",
    "RDD (Resilient Distributed Datasets) - это основная абстракция в Apache Spark. RDD представляет собой неизменяемую распределенную коллекцию объектов, которая может обрабатываться параллельно. В этом ноутбуке мы рассмотрим основные концепции и операции с RDD в PySpark.\n",
    "\n",
    "## Содержание\n",
    "1. Инициализация Spark и создание SparkContext\n",
    "2. Создание RDD\n",
    "3. Базовые операции с RDD\n",
    "4. Трансформации RDD\n",
    "5. Actions (действия) над RDD\n",
    "6. Парные RDD и операции с ключами\n",
    "7. Сохранение и загрузка RDD\n",
    "8. Практические примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Инициализация Spark и создание SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark numpy matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание SparkConf и SparkContext\n",
    "conf = SparkConf().setAppName(\"RDD Practice 2\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Проверка версии Spark\n",
    "print(f\"Spark Version: {sc.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание RDD\n",
    "Существует несколько способов создания RDD в Spark:\n",
    "\n",
    "### Создание RDD из коллекции Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание RDD из Python-списка\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "print(f\"RDD: {rdd}\")\n",
    "print(f\"RDD Content: {rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание RDD из внешних файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим текстовый файл для примера\n",
    "!echo \"Hello Spark\" > example.txt\n",
    "!echo \"RDD is fundamental abstraction in Spark\" >> example.txt\n",
    "!echo \"It represents a resilient distributed dataset\" >> example.txt\n",
    "# если выполняем на кластере, то используем hdfs\n",
    "!hdfs dfs -ls /user\n",
    "!hdfs dfs -mkdir -p /user/ubuntu\n",
    "!hadoop fs -put -f example.txt /user/ubuntu/example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание RDD из текстового файла\n",
    "text_rdd = sc.textFile(\"example.txt\")\n",
    "print(f\"Text RDD Content: {text_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание RDD с определенным числом партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание RDD с 4 партициями\n",
    "data_partitioned = sc.parallelize(range(10), 4)\n",
    "print(f\"Number of partitions: {data_partitioned.getNumPartitions()}\")\n",
    "print(f\"Partitioned RDD content: {data_partitioned.collect()}\")\n",
    "print(f\"Partitioned RDD glom: {data_partitioned.glom().collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Базовые операции с RDD\n",
    "\n",
    "### Информация об RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание RDD из Python-списка\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "print(f\"RDD: {rdd}\")\n",
    "print(f\"RDD Content: {rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение информации о партициях\n",
    "num_partitions = rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "# Оценка размера RDD\n",
    "print(f\"RDD count: {rdd.count()}\")\n",
    "\n",
    "# Просмотр содержимого партиций с помощью glom()\n",
    "print(f\"Content of each partition: {rdd.glom().collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Трансформации RDD\n",
    "Трансформации создают новый RDD из существующего. Они являются \"ленивыми\", то есть вычисляются только при вызове действия.\n",
    "\n",
    "### map() - применение функции к каждому элементу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение функции map для возведения в квадрат\n",
    "squared_rdd = rdd.map(lambda x: x**2)\n",
    "print(f\"Original RDD: {rdd.collect()}\")\n",
    "print(f\"Squared RDD: {squared_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter() - фильтрация элементов по условию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтрация четных чисел\n",
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(f\"Original RDD: {rdd.collect()}\")\n",
    "print(f\"Even numbers only: {even_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap() - генерация 0 или более выходных элементов из каждого входного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение строк на слова с помощью flatMap\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
    "print(f\"Original text RDD: {text_rdd.collect()}\")\n",
    "print(f\"Words RDD (after flatMap): {words_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct() - удаление дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание RDD с дубликатами\n",
    "dup_data = [1, 2, 2, 3, 3, 3, 4, 5, 5]\n",
    "dup_rdd = sc.parallelize(dup_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных элементов\n",
    "distinct_rdd = dup_rdd.distinct()\n",
    "print(f\"RDD with duplicates: {dup_rdd.collect()}\")\n",
    "print(f\"RDD after distinct(): {distinct_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample() - выборка элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выборка с заменой\n",
    "sample_with_replacement = rdd.sample(True, 0.5, seed=42)\n",
    "print(f\"Sample with replacement: {sample_with_replacement.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выборка без замены\n",
    "sample_without_replacement = rdd.sample(False, 0.5, seed=42)\n",
    "print(f\"Sample without replacement: {sample_without_replacement.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union(), intersection(), subtract() - теоретико-множественные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание второго RDD\n",
    "rdd2 = sc.parallelize([3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение RDD\n",
    "union_rdd = rdd.union(rdd2)\n",
    "print(f\"RDD1: {rdd.collect()}\")\n",
    "print(f\"RDD2: {rdd2.collect()}\")\n",
    "print(f\"Union: {union_rdd.collect()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пересечение RDD\n",
    "intersection_rdd = rdd.intersection(rdd2)\n",
    "print(f\"Intersection: {intersection_rdd.collect()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разность RDD\n",
    "subtract_rdd = rdd.subtract(rdd2)\n",
    "print(f\"RDD1 - RDD2: {subtract_rdd.collect()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Actions (действия) над RDD\n",
    "Действия возвращают значения из RDD в программу драйвера или записывают их во внешнюю систему хранения. Они запускают вычисления.\n",
    "\n",
    "### collect() - получение всех элементов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сбор всех элементов RDD в драйвер\n",
    "collected_data = rdd.collect()\n",
    "print(f\"Collected data: {collected_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count() - подсчет элементов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчет количества элементов\n",
    "count = rdd.count()\n",
    "print(f\"Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first() - получение первого элемента\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение первого элемента\n",
    "first_element = rdd.first()\n",
    "print(f\"First element: {first_element}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(n) - получение n элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение первых n элементов\n",
    "first_3 = rdd.take(3)\n",
    "print(f\"First 3 elements: {first_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce() - агрегирование элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all elements: 15\n",
      "Product of all elements: 120\n"
     ]
    }
   ],
   "source": [
    "# Сумма всех элементов с помощью reduce\n",
    "sum_of_elements = rdd.reduce(lambda a, b: a + b)\n",
    "print(f\"Sum of all elements: {sum_of_elements}\")\n",
    "\n",
    "# Произведение всех элементов\n",
    "product_of_elements = rdd.reduce(lambda a, b: a * b)\n",
    "print(f\"Product of all elements: {product_of_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach() - выполнение действия для каждого элемента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод каждого элемента (в логи исполнителей, не в драйвер)\n",
    "rdd.foreach(lambda x: print(f\"Element: {x}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Парные RDD и операции с ключами\n",
    "Парные RDD - это RDD, элементами которого являются пары (ключ, значение). Они поддерживают особые операции, такие как groupByKey, reduceByKey и др.\n",
    "\n",
    "### Создание парного RDD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs RDD: [('a', 1), ('b', 2), ('a', 3), ('c', 4), ('b', 5), ('c', 6)]\n"
     ]
    }
   ],
   "source": [
    "# Создание парного RDD\n",
    "pairs = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"c\", 4), (\"b\", 5), (\"c\", 6)]\n",
    "pairs_rdd = sc.parallelize(pairs)\n",
    "print(f\"Pairs RDD: {pairs_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey() - агрегирование значений по ключу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum by key: [('b', 7), ('c', 10), ('a', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Суммирование значений по ключу\n",
    "sums_by_key = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(f\"Sum by key: {sums_by_key.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey() - группировка значений по ключу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped by key: [('b', [2, 5]), ('c', [4, 6]), ('a', [1, 3])]\n"
     ]
    }
   ],
   "source": [
    "# Группировка значений по ключу\n",
    "grouped_by_key = pairs_rdd.groupByKey().mapValues(list)\n",
    "print(f\"Grouped by key: {grouped_by_key.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey() - сортировка по ключу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by key: [('a', 1), ('a', 3), ('b', 2), ('b', 5), ('c', 4), ('c', 6)]\n",
      "Sorted by key (descending): [('c', 4), ('c', 6), ('b', 2), ('b', 5), ('a', 1), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Сортировка по ключу\n",
    "sorted_by_key = pairs_rdd.sortByKey()\n",
    "print(f\"Sorted by key: {sorted_by_key.collect()}\")\n",
    "\n",
    "# Сортировка по ключу в обратном порядке\n",
    "sorted_by_key_desc = pairs_rdd.sortByKey(ascending=False)\n",
    "print(f\"Sorted by key (descending): {sorted_by_key_desc.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join() - соединение парных RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание второго парного RDD\n",
    "pairs2 = [(\"a\", \"apple\"), (\"b\", \"banana\"), (\"c\", \"cherry\")]\n",
    "pairs_rdd2 = sc.parallelize(pairs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD1: [('a', 1), ('b', 2), ('a', 3), ('c', 4), ('b', 5), ('c', 6)]\n",
      "RDD2: [('a', 'apple'), ('b', 'banana'), ('c', 'cherry')]\n",
      "Joined RDD: [('a', (1, 'apple')), ('a', (3, 'apple')), ('b', (2, 'banana')), ('b', (5, 'banana')), ('c', (4, 'cherry')), ('c', (6, 'cherry'))]\n"
     ]
    }
   ],
   "source": [
    "# Внутреннее соединение\n",
    "joined_rdd = pairs_rdd.join(pairs_rdd2)\n",
    "print(f\"RDD1: {pairs_rdd.collect()}\")\n",
    "print(f\"RDD2: {pairs_rdd2.collect()}\")\n",
    "print(f\"Joined RDD: {joined_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left joined: [('a', (1, 'apple')), ('a', (3, 'apple')), ('b', (2, 'banana')), ('b', (5, 'banana')), ('c', (4, 'cherry')), ('c', (6, 'cherry'))]\n"
     ]
    }
   ],
   "source": [
    "# Левое внешнее соединение\n",
    "left_joined = pairs_rdd.leftOuterJoin(pairs_rdd2)\n",
    "print(f\"Left joined: {left_joined.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right joined: [('a', (1, 'apple')), ('a', (3, 'apple')), ('b', (2, 'banana')), ('b', (5, 'banana')), ('c', (4, 'cherry')), ('c', (6, 'cherry'))]\n"
     ]
    }
   ],
   "source": [
    "# Правое внешнее соединение\n",
    "right_joined = pairs_rdd.rightOuterJoin(pairs_rdd2)\n",
    "print(f\"Right joined: {right_joined.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Сохранение и загрузка RDD\n",
    "\n",
    "### Сохранение в текстовый файл\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o718.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://rc1a-dataproc-m-op86a0r1gkd8750i.mdb.yandexcloud.net/user/ubuntu/key_sums.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:294)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-efe566d79e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Сохранение RDD в текстовый файл\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msums_by_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key_sums.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o718.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://rc1a-dataproc-m-op86a0r1gkd8750i.mdb.yandexcloud.net/user/ubuntu/key_sums.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:294)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Сохранение RDD в текстовый файл\n",
    "sums_by_key.saveAsTextFile(\"key_sums.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 52\n",
      "drwxr-xr-x 7 ubuntu ubuntu 4096 Jun 10 18:22 .\n",
      "drwxr-xr-x 4 root   root   4096 Jun 10 16:55 ..\n",
      "-rw------- 1 ubuntu ubuntu    5 Jun 10 18:17 .bash_history\n",
      "-rw-r--r-- 1 ubuntu ubuntu  220 Feb 25  2020 .bash_logout\n",
      "-rw-r--r-- 1 ubuntu ubuntu 3933 Jun  4 19:32 .bashrc\n",
      "drwx------ 5 ubuntu ubuntu 4096 Jun 10 18:21 .cache\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Jun 10 18:21 .config\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   98 Jun 10 18:22 example.txt\n",
      "drwxr-xr-x 5 ubuntu ubuntu 4096 Jun 10 18:20 .ipython\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Jun 10 18:20 .local\n",
      "-rw-r--r-- 1 ubuntu ubuntu  807 Feb 25  2020 .profile\n",
      "drwx------ 2 ubuntu ubuntu 4096 Jun  4 19:22 .ssh\n",
      "-rw-r--r-- 1 ubuntu ubuntu    0 Jun  4 19:22 .sudo_as_admin_successful\n",
      "-rwxr-xr-x 1 ubuntu ubuntu 1416 Jun 10 17:02 upload_data_to_hdfs.sh\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/user/ubuntu/key_sums': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Просмотр сохраненных файлов\n",
    "!ls -l key_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 'key_sums/part-*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Просмотр содержимого сохраненных файлов\n",
    "!cat key_sums/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление созданных файлов\n",
    "!rm -rf key_sums/\n",
    "!rm example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48\n",
      "drwxr-xr-x 7 ubuntu ubuntu 4096 Jun 10 18:34 .\n",
      "drwxr-xr-x 4 root   root   4096 Jun 10 16:55 ..\n",
      "-rw------- 1 ubuntu ubuntu    5 Jun 10 18:17 .bash_history\n",
      "-rw-r--r-- 1 ubuntu ubuntu  220 Feb 25  2020 .bash_logout\n",
      "-rw-r--r-- 1 ubuntu ubuntu 3933 Jun  4 19:32 .bashrc\n",
      "drwx------ 5 ubuntu ubuntu 4096 Jun 10 18:21 .cache\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Jun 10 18:21 .config\n",
      "drwxr-xr-x 5 ubuntu ubuntu 4096 Jun 10 18:20 .ipython\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Jun 10 18:20 .local\n",
      "-rw-r--r-- 1 ubuntu ubuntu  807 Feb 25  2020 .profile\n",
      "drwx------ 2 ubuntu ubuntu 4096 Jun  4 19:22 .ssh\n",
      "-rw-r--r-- 1 ubuntu ubuntu    0 Jun  4 19:22 .sudo_as_admin_successful\n",
      "-rwxr-xr-x 1 ubuntu ubuntu 1416 Jun 10 17:02 upload_data_to_hdfs.sh\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Практические примеры\n",
    "\n",
    "### Пример 1: Подсчет слов в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим текстовый файл с примером текста\n",
    "text = \"\"\"\n",
    "Apache Spark is an open-source unified analytics engine for large-scale data processing.\n",
    "Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"spark_text.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put -f spark_text.txt /user/ubuntu/spark_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка текста в RDD\n",
    "text_rdd = sc.textFile(\"spark_text.txt\")\n",
    "\n",
    "# Подсчет слов (классическая задача WordCount)\n",
    "word_counts = (text_rdd\n",
    "    .flatMap(lambda line: line.split())\n",
    "    .map(lambda word: (word.lower().strip(\".,;:\\\"\\'()[]{}\"), 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Количество слов в тексте:\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Визуализация топ-10 слов\n",
    "top_10_words = word_counts.take(10)\n",
    "words = [word for word, count in top_10_words]\n",
    "counts = [count for word, count in top_10_words]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel('Слова')\n",
    "plt.ylabel('Количество')\n",
    "plt.title('Топ-10 слов в тексте')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2: Оценка числа π методом Монте-Карло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside_circle(p):\n",
    "    \"\"\"Проверяет, находится ли точка p внутри единичной окружности\"\"\"\n",
    "    x, y = p\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# Число точек для оценки\n",
    "num_samples = 1000000\n",
    "\n",
    "# Генерация случайных точек\n",
    "samples = sc.parallelize([\n",
    "    (random.uniform(-1, 1), random.uniform(-1, 1)) \n",
    "    for _ in range(num_samples)\n",
    "])\n",
    "\n",
    "# Подсчет точек внутри окружности\n",
    "count_inside = samples.filter(inside_circle).count()\n",
    "\n",
    "# Оценка числа π\n",
    "pi_estimate = 4.0 * count_inside / num_samples\n",
    "\n",
    "print(f\"Оценка числа π методом Монте-Карло: {pi_estimate}\")\n",
    "print(f\"Истинное значение π: {np.pi}\")\n",
    "print(f\"Погрешность: {abs(pi_estimate - np.pi)}\")\n",
    "\n",
    "# Визуализация метода Монте-Карло (на меньшем числе точек)\n",
    "visualize_samples = 1000\n",
    "points = [(random.uniform(-1, 1), random.uniform(-1, 1)) for _ in range(visualize_samples)]\n",
    "inside = [p for p in points if inside_circle(p)]\n",
    "outside = [p for p in points if not inside_circle(p)]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter([p[0] for p in inside], [p[1] for p in inside], color='blue', s=3, label='Inside')\n",
    "plt.scatter([p[0] for p in outside], [p[1] for p in outside], color='red', s=3, label='Outside')\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='black')\n",
    "plt.gca().add_patch(circle)\n",
    "plt.axis('equal')\n",
    "plt.xlim(-1.05, 1.05)\n",
    "plt.ylim(-1.05, 1.05)\n",
    "plt.legend()\n",
    "plt.title(f'Оценка числа π методом Монте-Карло: {4.0 * len(inside) / visualize_samples:.4f}')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка после работы\n",
    "!rm spark_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Остановка SparkContext\n",
    "sc.stop()\n",
    "\n",
    "print(\"RDD практика завершена!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
