{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark - Работа с Dataset\n",
    "\n",
    "## Введение в Dataset\n",
    "Dataset - это строго типизированный API для работы с данными в Spark. Он был внедрен в Spark 1.6 и представляет собой расширение DataFrame API, объединяющее преимущества как RDD (строгая типизация), так и DataFrame (оптимизация запросов через Catalyst).\n",
    "\n",
    "**Важно:** В PySpark Dataset API напрямую не доступен из-за особенностей динамической типизации Python. В PySpark DataFrame является эквивалентом Dataset[Row] в Scala/Java. Однако, понимание концепции Dataset важно для тех, кто работает в смешанных средах Java/Scala/Python.\n",
    "\n",
    "## Содержание\n",
    "1. Инициализация Spark и создание SparkSession\n",
    "2. Dataset в экосистеме Spark\n",
    "3. Типизация в PySpark\n",
    "4. Использование UDF для типизированной обработки данных\n",
    "5. Pandas UDF и vectorized UDF\n",
    "6. Практические примеры работы с типизированной обработкой данных\n",
    "7. Интеграция с Pandas через Pandas API на PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Инициализация Spark и создание SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install findspark pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.0.3\n"
     ]
    }
   ],
   "source": [
    "# Создание SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Dataset Practice\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Проверка версии Spark\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset в экосистеме Spark\n",
    "\n",
    "### `Dataset` vs `DataFrame` vs `RDD`\n",
    "\n",
    "\n",
    "Рассмотрим различия между Dataset, DataFrame и RDD.\n",
    "\n",
    "В Scala/Java:\n",
    "- RDD: Низкоуровневый API с полным контролем над данными. Операции не проверяются компилятором.\n",
    "- DataFrame: Распределенная коллекция данных, организованная в именованные столбцы. Эквивалент Dataset[Row].\n",
    "- Dataset: Строго типизированный API, проверяемый компилятором.\n",
    "\n",
    "В PySpark:\n",
    "- RDD: Также доступен и работает так же, как в Scala/Java.\n",
    "- DataFrame: Основной API для работы с данными.\n",
    "- Dataset: Отдельного Dataset API нет из-за динамической типизации Python.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим простой DataFrame для демонстрации\n",
    "data = [(\"John\", 28, \"Sales\"), \n",
    "        (\"Anna\", 34, \"Finance\"), \n",
    "        (\"Robert\", 42, \"IT\"), \n",
    "        (\"Maria\", 39, \"HR\")]\n",
    "columns = [\"name\", \"age\", \"department\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age|department|\n",
      "+------+---+----------+\n",
      "|  John| 28|     Sales|\n",
      "|  Anna| 34|   Finance|\n",
      "|Robert| 42|        IT|\n",
      "| Maria| 39|        HR|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создаем DataFrame в PySpark\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD content: [('John', 28, 'Sales'), ('Anna', 34, 'Finance'), ('Robert', 42, 'IT'), ('Maria', 39, 'HR')]\n"
     ]
    }
   ],
   "source": [
    "# Создаем RDD из того же набора данных\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(\"RDD content:\", rdd.take(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В Scala мы могли бы создать Dataset следующим образом:\n",
    "\n",
    "```scala\n",
    "case class Employee(name: String, age: Int, department: String)\n",
    "val dataset = spark.createDataset(data.map(row => Employee(row._1, row._2, row._3)))\n",
    "```\n",
    "\n",
    "В Python вместо Dataset мы используем класс, близкий к концепции типизации:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение класса Python для хранения данных\n",
    "class Employee:\n",
    "    def __init__(self, name, age, department):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.department = department\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Employee({self.name}, {self.age}, {self.department})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python объекты: [Employee(John, 28, Sales), Employee(Anna, 34, Finance), Employee(Robert, 42, IT), Employee(Maria, 39, HR)]\n"
     ]
    }
   ],
   "source": [
    "# Преобразование данных в список объектов Employee\n",
    "employees = [Employee(name, age, dept) for name, age, dept in data]\n",
    "print(\"Python объекты:\", employees)\n",
    "\n",
    "# Однако мы не можем напрямую создать Dataset из этих объектов в PySpark\n",
    "# Вместо этого мы создаем DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Типизация в PySpark\n",
    "\n",
    "### Использование схем для обеспечения типизации\n",
    "\n",
    "\n",
    "Хотя PySpark не имеет полноценного Dataset API, мы можем использовать схемы для \n",
    "обеспечения некоторого уровня типизации при работе с данными.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение схемы с явными типами\n",
    "employee_schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "+------+---+----------+\n",
      "|  name|age|department|\n",
      "+------+---+----------+\n",
      "|  John| 28|     Sales|\n",
      "|  Anna| 34|   Finance|\n",
      "|Robert| 42|        IT|\n",
      "| Maria| 39|        HR|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание DataFrame с заданной схемой\n",
    "typed_df = spark.createDataFrame(data, employee_schema)\n",
    "typed_df.printSchema()\n",
    "typed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при попытке вставить данные неправильного типа:\n",
      "field age: IntegerType can not accept object 'thirty' in type <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Попытка вставить данные неправильного типа\n",
    "try:\n",
    "    wrong_data = [(\"Alex\", \"thirty\", \"Marketing\")]  # Возраст должен быть числом\n",
    "    wrong_df = spark.createDataFrame(wrong_data, employee_schema)\n",
    "    wrong_df.show()\n",
    "except Exception as e:\n",
    "    print(\"Ошибка при попытке вставить данные неправильного типа:\")\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование `pandas.DataFrame` с типизацией\n",
    "\n",
    "Python имеет собственные механизмы для обеспечения типизации в pandas, \n",
    "которые можно использовать перед преобразованием в PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Типы данных в pandas DataFrame:\n",
      "name          string\n",
      "age            int32\n",
      "department    string\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Создание pandas DataFrame с явными типами\n",
    "pandas_df = pd.DataFrame(data, columns=columns)\n",
    "pandas_df['age'] = pandas_df['age'].astype('int32')\n",
    "pandas_df['name'] = pandas_df['name'].astype('string')\n",
    "pandas_df['department'] = pandas_df['department'].astype('string')\n",
    "\n",
    "print(\"Типы данных в pandas DataFrame:\")\n",
    "print(pandas_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Преобразование в PySpark DataFrame\n",
    "spark_df_from_pandas = spark.createDataFrame(pandas_df)\n",
    "spark_df_from_pandas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Использование UDF для типизированной обработки данных\n",
    "\n",
    "### Определение обычных UDF\n",
    "\n",
    "\n",
    "User Defined Functions (UDF) в PySpark позволяют нам применять пользовательскую \n",
    "логику к данным с определенной типизацией.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение UDF с указанием возвращаемого типа\n",
    "@udf(returnType=IntegerType())\n",
    "def calculate_bonus(age, salary=1000):\n",
    "    # Допустим, что бонус рассчитывается на основе возраста\n",
    "    return int(salary * (age / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----+\n",
      "|  name|age|department|bonus|\n",
      "+------+---+----------+-----+\n",
      "|  John| 28|     Sales|  280|\n",
      "|  Anna| 34|   Finance|  340|\n",
      "|Robert| 42|        IT|  420|\n",
      "| Maria| 39|        HR|  390|\n",
      "+------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Использование UDF в DataFrame\n",
    "df_with_bonus = typed_df.withColumn(\"bonus\", calculate_bonus(col(\"age\")))\n",
    "df_with_bonus.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF для категоризации сотрудников по возрасту\n",
    "@udf(returnType=StringType())\n",
    "def age_category(age):\n",
    "    if age < 30:\n",
    "        return \"Young\"\n",
    "    elif age < 40:\n",
    "        return \"Mid-career\"\n",
    "    else:\n",
    "        return \"Experienced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----+-----------+\n",
      "|  name|age|department|bonus|   category|\n",
      "+------+---+----------+-----+-----------+\n",
      "|  John| 28|     Sales|  280|      Young|\n",
      "|  Anna| 34|   Finance|  340| Mid-career|\n",
      "|Robert| 42|        IT|  420|Experienced|\n",
      "| Maria| 39|        HR|  390| Mid-career|\n",
      "+------+---+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_category = df_with_bonus.withColumn(\"category\", age_category(col(\"age\")))\n",
    "df_with_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pandas UDF и vectorized UDF\n",
    "\n",
    "### Векторизованные UDF\n",
    "\n",
    "Pandas UDF (векторизованные UDF) позволяют более эффективно обрабатывать данные,  \n",
    "используя векторизацию и оптимизированные библиотеки Python, такие как NumPy и Pandas.  \n",
    "Они предлагают значительное повышение производительности по сравнению с обычными UDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек для Pandas UDF\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Создание DataFrame с большим количеством строк для демонстрации преимуществ\n",
    "large_data = [\n",
    "    (f\"Person-{i}\", 20 + i % 40, [\"Skill-1\", \"Skill-2\"][i % 2]) \n",
    "    for i in range(10000)\n",
    "]\n",
    "large_df = spark.createDataFrame(large_data, [\"name\", \"age\", \"skill\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скалярный Pandas UDF\n",
    "@pandas_udf(DoubleType())\n",
    "def pandas_calculate_bonus(age_series):\n",
    "    # Эта функция применяется к целым сериям pandas, а не к отдельным значениям\n",
    "    return age_series * 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+-----+\n",
      "|    name|age|  skill|bonus|\n",
      "+--------+---+-------+-----+\n",
      "|Person-0| 20|Skill-1|210.0|\n",
      "|Person-1| 21|Skill-2|220.5|\n",
      "|Person-2| 22|Skill-1|231.0|\n",
      "|Person-3| 23|Skill-2|241.5|\n",
      "|Person-4| 24|Skill-1|252.0|\n",
      "+--------+---+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Применение Pandas UDF\n",
    "large_df_with_bonus = large_df.withColumn(\"bonus\", pandas_calculate_bonus(col(\"age\")))\n",
    "large_df_with_bonus.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Практические примеры работы с типизированной обработкой данных\n",
    "\n",
    "### Пример: Анализ данных о сотрудниках\n",
    "\n",
    "Рассмотрим более комплексный пример обработки данных о сотрудниках \n",
    "с использованием типизированных конструкций.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание файла с данными о сотрудниках\n",
    "employees_data = \"\"\"\n",
    "id,name,age,department,salary,hire_date,performance_scores\n",
    "1,John Doe,32,IT,65000,2019-05-15,\"{'technical': 85, 'communication': 78, 'teamwork': 90}\"\n",
    "2,Jane Smith,28,Marketing,58000,2020-03-10,\"{'creativity': 92, 'communication': 88, 'organization': 85}\"\n",
    "3,Michael Brown,41,Finance,75000,2015-11-20,\"{'analytical': 95, 'attention': 89, 'reporting': 92}\"\n",
    "4,Emily Davis,35,HR,62000,2018-07-05,\"{'interpersonal': 94, 'organization': 91, 'communication': 87}\"\n",
    "5,Robert Wilson,39,IT,70000,2017-01-15,\"{'technical': 90, 'problem_solving': 88, 'innovation': 85}\"\n",
    "6,Sarah Lee,31,Marketing,59500,2020-02-20,\"{'creativity': 89, 'market_research': 86, 'presentation': 92}\"\n",
    "7,David Martinez,45,Finance,78000,2014-09-12,\"{'analytical': 92, 'financial_modeling': 95, 'risk_assessment': 88}\"\n",
    "8,Lisa Anderson,33,HR,63500,2018-04-25,\"{'recruitment': 91, 'policy': 87, 'mediation': 93}\"\n",
    "9,Thomas Taylor,37,IT,68000,2016-08-30,\"{'technical': 87, 'architecture': 92, 'security': 89}\"\n",
    "10,Jennifer White,29,Marketing,57000,2020-05-18,\"{'social_media': 94, 'content': 90, 'analytics': 85}\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"employees_detailed.csv\", \"w\") as f:\n",
    "    f.write(employees_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put -f employees_detailed.csv /user/ubuntu/employees_detailed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для парсинга строки JSON с оценками производительности\n",
    "import json\n",
    "def parse_performance_scores(scores_str):\n",
    "    try:\n",
    "        return json.loads(scores_str.replace(\"'\", \"\\\"\"))\n",
    "    except:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регистрация UDF для парсинга JSON\n",
    "parse_scores_udf = udf(parse_performance_scores, MapType(StringType(), IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных с использованием схемы\n",
    "employee_detailed_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False),\n",
    "    StructField(\"hire_date\", DateType(), False),\n",
    "    StructField(\"performance_scores\", StringType(), True)\n",
    "])\n",
    "\n",
    "employees_detailed_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(employee_detailed_schema) \\\n",
    "    .csv(\"employees_detailed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- performance_scores: string (nullable = true)\n",
      " |-- parsed_scores: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n",
      "+---+--------------+---+----------+------+----------+-------------------------------------------------------------------+-------------------------------------------------------------------+\n",
      "|id |name          |age|department|salary|hire_date |performance_scores                                                 |parsed_scores                                                      |\n",
      "+---+--------------+---+----------+------+----------+-------------------------------------------------------------------+-------------------------------------------------------------------+\n",
      "|1  |John Doe      |32 |IT        |65000 |2019-05-15|{'technical': 85, 'communication': 78, 'teamwork': 90}             |[technical -> 85, communication -> 78, teamwork -> 90]             |\n",
      "|2  |Jane Smith    |28 |Marketing |58000 |2020-03-10|{'creativity': 92, 'communication': 88, 'organization': 85}        |[communication -> 88, creativity -> 92, organization -> 85]        |\n",
      "|3  |Michael Brown |41 |Finance   |75000 |2015-11-20|{'analytical': 95, 'attention': 89, 'reporting': 92}               |[attention -> 89, analytical -> 95, reporting -> 92]               |\n",
      "|4  |Emily Davis   |35 |HR        |62000 |2018-07-05|{'interpersonal': 94, 'organization': 91, 'communication': 87}     |[communication -> 87, interpersonal -> 94, organization -> 91]     |\n",
      "|5  |Robert Wilson |39 |IT        |70000 |2017-01-15|{'technical': 90, 'problem_solving': 88, 'innovation': 85}         |[innovation -> 85, technical -> 90, problem_solving -> 88]         |\n",
      "|6  |Sarah Lee     |31 |Marketing |59500 |2020-02-20|{'creativity': 89, 'market_research': 86, 'presentation': 92}      |[presentation -> 92, creativity -> 89, market_research -> 86]      |\n",
      "|7  |David Martinez|45 |Finance   |78000 |2014-09-12|{'analytical': 92, 'financial_modeling': 95, 'risk_assessment': 88}|[risk_assessment -> 88, financial_modeling -> 95, analytical -> 92]|\n",
      "|8  |Lisa Anderson |33 |HR        |63500 |2018-04-25|{'recruitment': 91, 'policy': 87, 'mediation': 93}                 |[mediation -> 93, recruitment -> 91, policy -> 87]                 |\n",
      "|9  |Thomas Taylor |37 |IT        |68000 |2016-08-30|{'technical': 87, 'architecture': 92, 'security': 89}              |[security -> 89, technical -> 87, architecture -> 92]              |\n",
      "|10 |Jennifer White|29 |Marketing |57000 |2020-05-18|{'social_media': 94, 'content': 90, 'analytics': 85}               |[analytics -> 85, content -> 90, social_media -> 94]               |\n",
      "+---+--------------+---+----------+------+----------+-------------------------------------------------------------------+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Парсинг JSON столбца\n",
    "employees_detailed_df = employees_detailed_df \\\n",
    "    .withColumn(\"parsed_scores\", parse_scores_udf(col(\"performance_scores\")))\n",
    "\n",
    "employees_detailed_df.printSchema()\n",
    "employees_detailed_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение Pandas UDF для расчета средней оценки производительности\n",
    "@pandas_udf(DoubleType())\n",
    "def avg_performance(scores_series):\n",
    "    def calc_avg(scores_str):\n",
    "        scores = parse_performance_scores(scores_str)\n",
    "        if scores and len(scores) > 0:\n",
    "            # Простое математическое выражение вместо вызова sum()\n",
    "            total = 0\n",
    "            for value in scores.values():\n",
    "                total += value\n",
    "            return total / len(scores)\n",
    "        return 0.0\n",
    "    \n",
    "    return scores_series.apply(calc_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------------+\n",
      "|          name|department|  avg_performance|\n",
      "+--------------+----------+-----------------+\n",
      "|      John Doe|        IT|84.33333333333333|\n",
      "|    Jane Smith| Marketing|88.33333333333333|\n",
      "| Michael Brown|   Finance|             92.0|\n",
      "|   Emily Davis|        HR|90.66666666666667|\n",
      "| Robert Wilson|        IT|87.66666666666667|\n",
      "|     Sarah Lee| Marketing|             89.0|\n",
      "|David Martinez|   Finance|91.66666666666667|\n",
      "| Lisa Anderson|        HR|90.33333333333333|\n",
      "| Thomas Taylor|        IT|89.33333333333333|\n",
      "|Jennifer White| Marketing|89.66666666666667|\n",
      "+--------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Применение Pandas UDF\n",
    "employees_with_avg_score = employees_detailed_df \\\n",
    "    .withColumn(\"avg_performance\", avg_performance(col(\"performance_scores\")))\n",
    "\n",
    "employees_with_avg_score.select(\"name\", \"department\", \"avg_performance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  department    avg_salary  min_salary  max_salary    avg_age  avg_performance\n",
      "0    Finance  76500.000000       75000       78000  43.000000        91.833333\n",
      "1         HR  62750.000000       62000       63500  34.000000        90.500000\n",
      "2         IT  67666.666667       65000       70000  36.000000        87.111111\n",
      "3  Marketing  58166.666667       57000       59500  29.333333        89.000000\n"
     ]
    }
   ],
   "source": [
    "# Анализ по отделам с использованием Pandas API\n",
    "\n",
    "# Преобразование Spark DataFrame в Pandas-on-Spark DataFrame\n",
    "psdf = employees_with_avg_score.toPandas()\n",
    "\n",
    "# Анализ по отделам\n",
    "department_analysis = psdf.groupby(\"department\").agg({\n",
    "    \"salary\": [\"mean\", \"min\", \"max\"],\n",
    "    \"age\": \"mean\",\n",
    "    \"avg_performance\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "department_analysis.columns = [\"department\", \"avg_salary\", \"min_salary\", \"max_salary\", \"avg_age\", \"avg_performance\"]\n",
    "print(department_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Интеграция с Pandas через Pandas API на PySpark\n",
    "\n",
    "### Pandas API на PySpark\n",
    "\n",
    "PySpark предоставляет Pandas API, который позволяет использовать знакомый интерфейс \n",
    "Pandas для работы с большими данными. Это близко подходит к концепции Dataset \n",
    "с точки зрения удобства работы и типизации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверить локально на последней версии PySpark 3.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame:\n",
      "   id     name  age department\n",
      "0   1    Alice   25         IT\n",
      "1   2      Bob   30         HR\n",
      "2   3  Charlie   35    Finance\n",
      "3   4    David   40         IT\n",
      "4   5    Ellen   45  Marketing\n"
     ]
    }
   ],
   "source": [
    "# Импорт Pandas API на PySpark\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Создание Pandas на Spark DataFrame\n",
    "ps_df = ps.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Ellen'],\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'department': ['IT', 'HR', 'Finance', 'IT', 'Marketing']\n",
    "})\n",
    "\n",
    "print(\"Pandas на Spark DataFrame:\")\n",
    "print(ps_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фильтрация данных:\n",
      "   id     name  age department\n",
      "2   3  Charlie   35    Finance\n",
      "3   4    David   40         IT\n",
      "4   5    Ellen   45  Marketing\n",
      "\n",
      "Группировка данных:\n",
      "             age\n",
      "department      \n",
      "Finance     35.0\n",
      "HR          30.0\n",
      "IT          32.5\n",
      "Marketing   45.0\n"
     ]
    }
   ],
   "source": [
    "# Базовые операции, похожие на Pandas\n",
    "print(\"\\nФильтрация данных:\")\n",
    "print(ps_df[ps_df['age'] > 30])\n",
    "\n",
    "print(\"\\nГруппировка данных:\")\n",
    "print(ps_df.groupby('department').agg({'age': 'mean'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование в обычный Spark DataFrame\n",
    "spark_df = ps_df.to_spark()\n",
    "print(\"\\nПреобразованный Spark DataFrame:\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ono/otus/de/otus-de-spark-api/.venv/lib/python3.11/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обычный Pandas DataFrame:\n",
      "   id     name  age department\n",
      "0   1    Alice   25         IT\n",
      "1   2      Bob   30         HR\n",
      "2   3  Charlie   35    Finance\n",
      "3   4    David   40         IT\n",
      "4   5    Ellen   45  Marketing\n"
     ]
    }
   ],
   "source": [
    "# Преобразование обратно в Pandas (для небольших данных)\n",
    "regular_pandas_df = ps_df.to_pandas()\n",
    "print(\"\\nОбычный Pandas DataFrame:\")\n",
    "print(regular_pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка ресурсов\n",
    "!rm employees_detailed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset и типизированная обработка данных в PySpark практика завершена!\n"
     ]
    }
   ],
   "source": [
    "# Остановка SparkSession\n",
    "spark.stop()\n",
    "\n",
    "print(\"Dataset и типизированная обработка данных в PySpark практика завершена!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
